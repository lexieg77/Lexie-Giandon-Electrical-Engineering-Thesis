{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b80ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES #\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import fitz\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2c98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE MODEL #\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"o3-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0681a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV FILES #\n",
    "onedrive_link1 = r\"C:/Users/lgian/OneDrive - James Cook University/Data/Sample_Spare_Parts2.csv\"\n",
    "non_generic_parts = pd.read_csv(onedrive_link1)\n",
    "\n",
    "onedrive_link2 = r\"C:/Users/lgian/OneDrive - James Cook University/Data/Downtime_History_Data.csv\"\n",
    "downtime_history = pd.read_csv(onedrive_link2, encoding = \"cp1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02fb8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVAL #\n",
    "# Convert each part to a string\n",
    "parts_list = non_generic_parts.apply(\n",
    "    lambda row: f\"{row['CAT Part']}: {row['SAP Material Description']}\", axis = 1).tolist()\n",
    "\n",
    "failure_list = downtime_history.apply(\n",
    "    lambda row: f\"{row['Order']}: {row['Notification']}: {row['Order Type']}: {row['Order Long Text Description']}: {row['Notification Long Text Description']}: {row['Sort Field']}: {row['Total Costs']}: {row['Total Work Hours ']}\", axis = 1).tolist()\n",
    "\n",
    "# Split long descriptions into smaller chunks using key-value pairs\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "# Split parts into chunks\n",
    "parts_kv_store = {}\n",
    "parts_chunks = []\n",
    "\n",
    "for i, text in enumerate(non_generic_parts):\n",
    "    file_chunks = text_splitter.split_text(text)\n",
    "    for j, chunk in enumerate(file_chunks):\n",
    "        key = f\"part_{i}_chunk_{j}\"\n",
    "        parts_kv_store[key] = {\n",
    "            \"text\": chunk,\n",
    "            \"source\": f\"part_{i}\"\n",
    "        }\n",
    "        parts_chunks.append(chunk)\n",
    "\n",
    "# Split failures into chunks\n",
    "failure_kv_store = {}\n",
    "failure_chunks = []\n",
    "\n",
    "for i, text in enumerate(downtime_history):\n",
    "    file_chunks = text_splitter.split_text(text)\n",
    "    for j, chunk in enumerate(file_chunks):\n",
    "        key = f\"failure_{i}_chunk_{j}\"\n",
    "        failure_kv_store[key] = {\n",
    "            \"text\": chunk,\n",
    "            \"source\": f\"failure_{i}\"\n",
    "        }\n",
    "        failure_chunks.append(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "637421de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING #\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "k = 5  # Number of clusters\n",
    "\n",
    "embedding_parts_vectors = embedding_model.embed_documents(parts_chunks)\n",
    "embedding_parts_vectors = np.array(embedding_parts_vectors).astype('float32')\n",
    "parts_dimension = embedding_parts_vectors.shape[1] \n",
    "\n",
    "embedding_failure_vectors = embedding_model.embed_documents(failure_chunks)\n",
    "embedding_failure_vectors = np.array(embedding_failure_vectors).astype('float32')\n",
    "failure_dimension = embedding_failure_vectors.shape[1] \n",
    "\n",
    "# Create and train the KMeans model\n",
    "kmeans_parts = faiss.Kmeans(d=parts_dimension, k=k, niter=20, verbose=True)\n",
    "kmeans_parts.train(embedding_parts_vectors)\n",
    "\n",
    "kmeans_failure = faiss.Kmeans(d=failure_dimension, k=k, niter=20, verbose=True)\n",
    "kmeans_failure.train(embedding_failure_vectors)\n",
    "\n",
    "# Assign each embedding to a cluster\n",
    "parts_index = kmeans_parts.index\n",
    "failure_index = kmeans_failure.index\n",
    "\n",
    "# Assign each vector to the nearest centroid\n",
    "parts_distances, parts_cluster_ids = parts_index.search(embedding_parts_vectors, 1) # cluster_ids is shape with the cluster label for eahc vector\n",
    "failure_distances, failure_cluster_ids = failure_index.search(embedding_failure_vectors, 1) \n",
    "\n",
    "# Flatten if needed\n",
    "parts_cluster_ids = parts_cluster_ids.flatten()\n",
    "failure_cluster_ids = failure_cluster_ids.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc4e97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VECTORSTORES ##\n",
    "# Metadata now includes both cluster ID and key for exact retrieval\n",
    "parts_metadatas = [\n",
    "    {\"cluster\": int(cid), \"key\": key, \"source\": parts_kv_store[key][\"source\"]}\n",
    "    for cid, key in zip(parts_cluster_ids, parts_kv_store.keys())\n",
    "]\n",
    "\n",
    "failure_metadatas = [\n",
    "    {\"cluster\": int(cid), \"key\": key, \"source\": failure_kv_store[key][\"source\"]}\n",
    "    for cid, key in zip(failure_cluster_ids, failure_kv_store.keys())\n",
    "]\n",
    "\n",
    "# Add metadata with cluster IDs!\n",
    "parts_vectorstore = FAISS.from_texts(\n",
    "    texts=parts_chunks,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=parts_metadatas\n",
    ")\n",
    "\n",
    "failure_vectorstore = FAISS.from_texts(\n",
    "    texts=failure_chunks,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=failure_metadatas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09ed3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING PDFs #\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"  # Avoid None\n",
    "    return text\n",
    "\n",
    "# Dictionary of PDF names and paths\n",
    "pdf_files = {\n",
    "    \"KXE_brochure\": r\"C:/Users/lgian/OneDrive - James Cook University/Data/998KXE_Brochure.pdf\",\n",
    "    \"XE_brochure\": r\"C:/Users/lgian/OneDrive - James Cook University/Data/998XE_Brochure.pdf\",\n",
    "    \"XE_specs\": r\"C:/Users/lgian/OneDrive - James Cook University/Data/998XE_Specifications.pdf\",\n",
    "}\n",
    "\n",
    "# Extract text for each file\n",
    "pdf_texts = {name: extract_text_from_pdf(path) for name, path in pdf_files.items()}\n",
    "\n",
    "# Split into chunks\n",
    "splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "# Retreive exact chunks later by key\n",
    "pdf_kv_store = {} # key-value store: key = chunk ID, value = chunk text + metadata\n",
    "chunks = []\n",
    "for name, text in pdf_texts.items():\n",
    "    file_chunks = splitter.split_text(text)\n",
    "    for j, chunk in enumerate(file_chunks):\n",
    "        key = f\"{name}_chunk_{j}\"\n",
    "        pdf_kv_store[key] = {\n",
    "            \"text\": chunk,\n",
    "            \"source\": name\n",
    "        }\n",
    "        chunks.append(chunk)\n",
    "\n",
    "# Create embeddings + cluster using FAISS (kmeans)\n",
    "embedding_pdf_vectors = embedding_model.embed_documents(chunks)\n",
    "embedding_pdf_vectors = np.array(embedding_pdf_vectors).astype('float32')\n",
    "pdf_dimension = embedding_pdf_vectors.shape[1]  \n",
    "kmeans_pdf = faiss.Kmeans(d=pdf_dimension, k=k, niter=20, verbose=True)\n",
    "kmeans_pdf.train(embedding_pdf_vectors)\n",
    "pdf_index = kmeans_pdf.index\n",
    "pdf_distances, pdf_cluster_ids = pdf_index.search(embedding_pdf_vectors, 1)\n",
    "pdf_cluster_ids = pdf_cluster_ids.flatten()\n",
    "\n",
    "# Metadata includes both cluster ID and key for exact retrieval\n",
    "metadatas = [\n",
    "    {\"cluster\": int(cid), \"key\": key, \"source\": pdf_kv_store[key][\"source\"]}\n",
    "    for cid, key in zip(pdf_cluster_ids, pdf_kv_store.keys())\n",
    "]\n",
    "\n",
    "pdf_vectorstore = FAISS.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=metadatas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "206fbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPTING #\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are a reliability engineer. \n",
    "You have the following list of part numbers and descriptions:\n",
    "\n",
    "{parts_context}\n",
    "\n",
    "Task: For the given failure description, return the most relevant PartNumber.\n",
    "If none of the part numbers seem to fit, return \"Unknown\".\n",
    "\n",
    "Use this extra context to make informed decisions: \n",
    "{KXE_brochure_context}, {XE_brochure_context}, {XE_specs_context}\n",
    "\n",
    "Failure: {failure_context}\n",
    "Answer with ONLY the PartNumber or \"Unknown\".\n",
    "\n",
    "Please output your thinking process as well as your final answer.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5c24b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVER #\n",
    "# Each time the for loop below runs it will get the top 3 similair parts from the parts_vectorstore\n",
    "retriever_pdf = pdf_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "relevant_pdf_chunks = retriever_pdf.get_relevant_documents(text)\n",
    "pdf_context = \"\\n\".join([f\"{doc.metadata['key']}: {doc.page_content}\" for doc in relevant_pdf_chunks])\n",
    "\n",
    "parts_retriever = parts_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "relevant_parts_chunks = parts_retriever.get_relevant_documents(text)\n",
    "parts_context = \"\\n\".join([f\"{doc.metadata['key']}: {doc.page_content}\" for doc in relevant_parts_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9be009ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILURE MAPPING #\n",
    "failure_to_part = []\n",
    "spare_parts = []\n",
    "pdf = []\n",
    "\n",
    "for text in failure_list:   \n",
    "    # Prepare prompt\n",
    "    prompt = prompt_template.format(\n",
    "        parts_context= parts_context, \n",
    "        failure_context=text, \n",
    "        KXE_brochure_context=pdf_context,\n",
    "        XE_brochure_context=pdf_context,\n",
    "        XE_specs_context=pdf_context,\n",
    "    )\n",
    "    \n",
    "    # Get LLM response\n",
    "    response = llm.predict(prompt)\n",
    "    \n",
    "    failure_to_part.append({\n",
    "        \"Failure\": text,\n",
    "        \"Mapped Part\": response\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6d4ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_search = pd.DataFrame(failure_to_part)\n",
    "similarity_search.to_excel(r\"C:/Users/lgian/OneDrive - James Cook University/Data/Key_Value_Results.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
