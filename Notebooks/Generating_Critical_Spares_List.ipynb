{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNlT2IPWpPe5"
   },
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn langchain_core langchain_community langchain_openai langchain_text_splitters PyPDF2 faiss-cpu pymupdf --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21FTp8OUzG4p"
   },
   "outputs": [],
   "source": [
    "# LIBRARIES #\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import fitz\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from PyPDF2 import PdfReader\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import faiss\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1762496312464,
     "user": {
      "displayName": "Lexie Giandon",
      "userId": "07532099552372755131"
     },
     "user_tz": -600
    },
    "id": "EA1Isk4t0_zC",
    "outputId": "eb1112e6-2066-4d79-9be6-6eb465f11969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY environment variable set.\n"
     ]
    }
   ],
   "source": [
    "# LLM #\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "print(\"OPENAI_API_KEY environment variable set.\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"o3-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3155,
     "status": "ok",
     "timestamp": 1762496317463,
     "user": {
      "displayName": "Lexie Giandon",
      "userId": "07532099552372755131"
     },
     "user_tz": -600
    },
    "id": "TlMQATR60k_e",
    "outputId": "72a0674c-d228-4a71-cbfa-e9cef5845d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65244,
     "status": "ok",
     "timestamp": 1762496392584,
     "user": {
      "displayName": "Lexie Giandon",
      "userId": "07532099552372755131"
     },
     "user_tz": -600
    },
    "id": "ooGmJoIc0Z3r",
    "outputId": "cd231283-a386-4c84-f479-d8cff4de0da9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding PDF chunks: 100%|██████████| 278/278 [01:02<00:00,  4.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# READING PDFs #\n",
    "# Dictionary of PDF names and paths\n",
    "pdf_paths = {\n",
    "    \"KXE_brochure\": \"/content/drive/My Drive/Colab Notebooks/998KXE_Brochure.pdf\",\n",
    "    \"XE_brochure\": \"/content/drive/My Drive/Colab Notebooks/998XE_Brochure.pdf\",\n",
    "    \"XE_specs\": \"/content/drive/My Drive/Colab Notebooks/998XE_Specifications.pdf\",\n",
    "    \"risk_assessment\": \"/content/drive/My Drive/Colab Notebooks/Risk_Assessment.pdf\"\n",
    "}\n",
    "\n",
    "# Extract text from PDFs\n",
    "def extract_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path) # Open the PDF document\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text() # Extract text from each page\n",
    "    return text\n",
    "pdf_texts = {label: extract_text(path) for label, path in pdf_paths.items()}\n",
    "\n",
    "# Split texts into chunks with labels\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts, labels = [], []\n",
    "\n",
    "for label, text in pdf_texts.items():\n",
    "    chunks = splitter.split_text(text)\n",
    "    texts.extend(chunks)\n",
    "    labels.extend([label] * len(chunks))\n",
    "\n",
    "# Create embeddings\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "X = [embeddings_model.embed_query(t) for t in tqdm.tqdm(texts, desc=\"Embedding PDF chunks\")]\n",
    "X = np.array(X)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Supervised Classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Build FAISS vector stores for each PDF\n",
    "vectorstores = {}\n",
    "for label, text in pdf_texts.items():\n",
    "    chunks = splitter.split_text(text)\n",
    "    vectorstores[label] = FAISS.from_texts(chunks, embeddings_model)\n",
    "\n",
    "# labels - KXE_brochure, XE_brochure, XE_specs, risk_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBRSKiAw188z"
   },
   "outputs": [],
   "source": [
    "# CSV FILES #\n",
    "non_generic_parts_path = \"/content/drive/My Drive/Colab Notebooks/50_Spare_Parts.csv\"\n",
    "non_generic_parts = pd.read_csv(non_generic_parts_path)\n",
    "\n",
    "failure_path = \"/content/drive/My Drive/Colab Notebooks/Failure_Data.csv\"\n",
    "failures = pd.read_csv(failure_path, encoding = \"cp1252\")\n",
    "\n",
    "original_critical_list_path = \"/content/drive/My Drive/Colab Notebooks/Original_Critical_Spare_Parts_List.csv\"\n",
    "original_critical_spares_list = pd.read_csv(original_critical_list_path)\n",
    "\n",
    "# Convert each part to a string\n",
    "parts_list = non_generic_parts.apply(\n",
    "    lambda row: f\"{row['CAT Part']}: {row['SAP Material Description']}: {row['U801 MRP']}: {row['Unit Cost']}: {row['Lead Time']}\", axis = 1).tolist()\n",
    "\n",
    "failure_list = failures.apply(\n",
    "    lambda row: f\"{row['Order']}: {row['Notification']}: {row['Order Type']}: {row['Order Long Text Description']}: {row['Notification Long Text Description']}: {row['Sort field']}: {row['Total Costs']}: {row['Total Work Hours ']}\", axis = 1).tolist()\n",
    "all_failure_text = \"\\n\".join(failure_list)\n",
    "\n",
    "critical_list = original_critical_spares_list.apply(\n",
    "    lambda row: f\"{row['Material Number']}: {row['Description']}: {row['Failure mode']}: {row['Maintenance Strategy']}: {row['Expected lead time']}: {row['Is a repairable?']}: {row['Highest Potential Impact']}: {row['Highest Potential Impact Cost']}: {row['Likelihood']}: {row['Estimated annual usage']}: {row['Usage frequency']}: {row['Justification commentary']}\", axis = 1).tolist()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "failure_chunks = text_splitter.split_text(all_failure_text)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "failures_vectorstore = FAISS.from_texts(failure_chunks, embeddings)\n",
    "failures_retriever = failures_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyyX1XB-vvHI"
   },
   "outputs": [],
   "source": [
    "# PROMPTING #\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "STEP 1\n",
    "You are acting as a reliability engineer with expertise in mechanical engineering and the mining industry.\n",
    "Your task is to analyse historical failure data for two Caterpillar (CAT) 998KXE front-end loaders at the North Queensland Hard\n",
    "Rock Mine (NQHRM). Your goal is to produce a critical spare parts list for the 998KXE front-end loaders to minimise operational downtime\n",
    "and financial losses.\n",
    "\n",
    "STEP 2\n",
    "I have provided the official 998KXE brochure and the brochure/specifications of the older but similar 998XE model, which has more\n",
    "publicly available data online. Use both documents, plus any additional knowledge you have of CAT equipment to make well-informed decisions\n",
    "about spare parts:\n",
    "\n",
    "{KXE_brochure} {XE_brochure} {XE_specs}\n",
    "\n",
    "STEP 3\n",
    "A critical spares list identifies the essential parts that must be\n",
    "stocked on-site to minimise machine downtime. Parts are considered critical if:\n",
    "•       They have long lead times OR\n",
    "•       They are linked to high financial losses from machine downtime OR\n",
    "•       They contribute to unplanned downtime when they fail\n",
    "\n",
    "The critical spare parts list excludes parts already stocked on site (V1 classified parts) and generic parts such as nuts and bolts.\n",
    "Currently, the NQHRM is producing their critical spare parts lists manually and I have provided a sample of their list for the 998KXE\n",
    "front-end loader:\n",
    "\n",
    "{original_critical_spares_list}\n",
    "\n",
    "STEP 4\n",
    "You have been provided with two Excel files which you will use to generate the critical spare parts list:\n",
    "Non-Generic Spare Parts: {spare_parts}\n",
    "•       CAT Part: 7-digit part number from CAT. (Part number = CAT part = CAT part number)\n",
    "•       7-digit number (eg: 489-1166) OR\n",
    "•       6-digit number with the second digit being a letter (eg: 8T-9527)\n",
    "•       SAP Material Description: Short description of the CAT part number\n",
    "•       U801 MRP:\n",
    "•           ND: Marked for deletion (can be changed)\n",
    "•           PD: Catalogued but not stocked\n",
    "•           V1: Already stocked on-site\n",
    "•       Unit Cost: Part cost\n",
    "•       Lead Time: Expected delivery time from CAT or supplier to the mine\n",
    "\n",
    "Downtime History Data: {failures}\n",
    "•       Order number: 10-digit number corresponding to the order (linked to notification)\n",
    "•       Notification number: 10-digit number corresponding to the notification (linked to order)\n",
    "•       Order type:\n",
    "•           PM01: Corrective maintenance identified during scheduled servicing (non-urgent)\n",
    "•           PM02: Scheduled preventive maintenance (mostly labour)\n",
    "•           PM03: Emergency corrective maintenance for unexpected failures (immediate)\n",
    "•       Order long text description: Maintenance description\n",
    "•           Eg: “4W Mech Svce Bucket CAT 988KXE Loader 1” which means there is scheduled maintenance every four weeks on the CAT 988KXE Loader 1 bucket.\n",
    "•       Notification long text description: Failure details including timestamp, repair details and sometimes the CAT part number\n",
    "•       Sort field: Machine identifier (Loader 1 or Loader 2)\n",
    "•       Total costs: Cost of repairing/replacing the part inclusive of labour hire\n",
    "•       Total work hours: Downtime duration\n",
    "\n",
    "If data is missing, use 998XKE failure rates or industry benchmarks (eg: from CATs website) to fill in the gaps and document assumptions.\n",
    "\n",
    "While the two datasets are not directly linked, part matches can be made by:\n",
    "•       Comparing the notification long text with CAT part numbers\n",
    "•       Comparing the order long text with the SAP material description\n",
    "•       Applying domain knowledge of 998KXE/998XE systems\n",
    "\n",
    "\n",
    "STEP 5\n",
    "Criticality score = Likelihood score x Severity score\n",
    "•       Likelihood score: Frequency of failure occurrences. Calculate the failure count for PM03 records in the downtime history data.\n",
    "    Exclude PM01 and PM02 unless explicitly ties to a failure\n",
    "•       Severity Score: The financial impact of machine downtime. Calculate as the sum of unit cost (from non-generic parts list),\n",
    "    labour cost (from downtime history) and downtime cost ($10,400 per day x total work hours converted to days)\n",
    "\n",
    "Use the likelihood and severity risk tables provided to identify the scores of each spare part.\n",
    "Then use the provided risk assessment matrix to calculate the criticality score. For example, a low severity and unlikely\n",
    "likelihood results in a risk score of 0.3:\n",
    "\n",
    "{risk_assessment}\n",
    "\n",
    "Any part with a criticality score greater than 30 is classified as a critical spare part. Therefore, generate a critical spare parts list for the 998KXE front end-loader that follows the below structure\n",
    "•       CAT Part\n",
    "•       SAP Material Description\n",
    "•       U801 MRP\n",
    "•       Unit Cost (USD)\n",
    "•       Lead Time (days)\n",
    "•       Assumed Likelihood Score\n",
    "•       Estimated Severity Score\n",
    "•       Criticality Score\n",
    "•       Comments\n",
    "\n",
    "STEP 6\n",
    "Before finalising the critical spare parts list, verify:\n",
    "•       All risk scores greater than 30 are included\n",
    "•       V1 classified parts are excluded\n",
    "•       Calculations align with provided matrices\n",
    "•       Part numbers are valid  CAT numbers\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbit5YkFImYJ"
   },
   "outputs": [],
   "source": [
    "## RETRIEVAL ##\n",
    "# Process each spare part\n",
    "critical_spares_list = []\n",
    "embedding_cache = {}\n",
    "\n",
    "def get_cached_embedding(text):\n",
    "    if text not in embedding_cache:\n",
    "        embedding_cache[text] = embeddings_model.embed_query(text)\n",
    "    return embedding_cache[text]\n",
    "\n",
    "for part in parts_list:\n",
    "    part_emb = get_cached_embedding(part)\n",
    "    predicted_label = clf.predict([part_emb])[0]\n",
    "\n",
    "    # Retrieve top 5 relevant chunks from predicted PDF\n",
    "    retriever = vectorstores[predicted_label].as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "    relevant_docs = retriever.invoke(part)\n",
    "    context_snippets = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    # Feed into LLM prompt\n",
    "    prompt_value = prompt.format(\n",
    "        spare_parts=parts_list,\n",
    "        failures=failure_list,\n",
    "        KXE_brochure=context_snippets if \"KXE_brochure\" == predicted_label else \"\",\n",
    "        XE_brochure=context_snippets if \"XE_brochure\" == predicted_label else \"\",\n",
    "        XE_specs=context_snippets if \"XE_specs\" == predicted_label else \"\",\n",
    "        original_critical_spares_list=\"\\n\".join(critical_list),\n",
    "        risk_assessment=context_snippets if \"risk_assessment\" == predicted_label else \"\",\n",
    "\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(prompt_value)\n",
    "\n",
    "    critical_spares_list.append({\n",
    "        \"Critical Spare Parts List\": response\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "critical_spares_list_df = pd.DataFrame(critical_spares_list)\n",
    "output_path = \"/content/drive/MyDrive/Colab Notebooks/critical_spares_list.csv\"\n",
    "critical_spares_list_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qI91kHDAI4KG"
   },
   "outputs": [],
   "source": [
    "## FORMATTING RESULTS ##\n",
    "# Load text\n",
    "with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Normalize newlines (replace literal \\n with actual newline)\n",
    "text = text.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "# Split by numbered entries (more flexible)\n",
    "entries = re.split(r\"\\n\\d+\\.\\s+\", text)\n",
    "entries = [e.strip() for e in entries if e.strip()]\n",
    "\n",
    "data = []\n",
    "\n",
    "keys = [\"CAT Part\", \"SAP Material Description\", \"U801 MRP\",\n",
    "        \"Unit Cost (USD)\", \"Lead Time (days)\", \"Assumed Likelihood Score\",\n",
    "        \"Estimated Severity Score\", \"Criticality Score\", \"Comments\"]\n",
    "\n",
    "for entry in entries:\n",
    "    entry_dict = {}\n",
    "\n",
    "    # Remove extra sections\n",
    "    entry = re.split(r\"Verification checks:|Notes and Verification Steps:|Assumptions and verification\", entry)[0]\n",
    "\n",
    "    # Clean up formatting\n",
    "    entry = re.sub(r\"[•\\u2003\\u2022─]+\", \" \", entry)\n",
    "    entry = re.sub(r\"\\\\n\\\\u2003|\\\\u2003\\\\u2003|\\\\n\\\\n\", \" \", entry)\n",
    "    entry = re.sub(r\"\\\\u2003\", \"\", entry)\n",
    "    entry = re.sub(r\"\\[.*?\\]\", \"\", entry)\n",
    "    entry = re.sub(r\"\\s+\", \" \", entry).strip()\n",
    "\n",
    "    # Extract key-value pairs\n",
    "    for i, key in enumerate(keys):\n",
    "        if i < len(keys) - 1:\n",
    "            next_key = keys[i+1]\n",
    "            pattern = f\"{re.escape(key)}: (.*?)(?={re.escape(next_key)}:|$)\"\n",
    "        else:\n",
    "            pattern = f\"{re.escape(key)}: (.*)\"\n",
    "        match = re.search(pattern, entry)\n",
    "        entry_dict[key] = match.group(1).strip() if match else \"\"\n",
    "\n",
    "    data.append(entry_dict)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"/content/drive/MyDrive/Colab Notebooks/formatted_critical_list.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1ZZ5Bb9-M4MiDcfNMLXQJ38Ks1nmRUWbg",
     "timestamp": 1763555789458
    },
    {
     "file_id": "1Z900xh48Qd91Uq43dajU1ssOndKhS0_G",
     "timestamp": 1760687829214
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
