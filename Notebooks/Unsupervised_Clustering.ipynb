{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES #\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import fitz\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac2c98ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING THE MODEL #\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"o4-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0681a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV FILES #\n",
    "onedrive_link1 = r\"C:/Users/lgian/OneDrive - James Cook University/Data/Sample_Spare_Parts2.csv\"\n",
    "non_generic_parts = pd.read_csv(onedrive_link1)\n",
    "\n",
    "onedrive_link2 = r\"C:/Users/lgian/OneDrive - James Cook University/Data/Downtime_History_Data.csv\"\n",
    "downtime_history = pd.read_csv(onedrive_link2, encoding = \"cp1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02fb8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVAL #\n",
    "# Convert each part to a string\n",
    "parts_list = non_generic_parts.apply(\n",
    "    lambda row: f\"{row['CAT Part']}: {row['SAP Material Description']}\", axis = 1).tolist()\n",
    "\n",
    "failure_list = downtime_history.apply(\n",
    "    lambda row: f\"{row['Order']}: {row['Notification']}: {row['Order Type']}: {row['Order Long Text Description']}: {row['Notification Long Text Description']}: {row['Sort Field']}: {row['Total Costs']}: {row['Total Work Hours ']}\", axis = 1).tolist()\n",
    "\n",
    "# Split long descriptions into smaller chunks\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "parts_chunks = []\n",
    "for text in parts_list:\n",
    "    parts_chunks.extend(text_splitter.split_text(text))\n",
    "\n",
    "failure_chunks = []\n",
    "for text in failure_list:\n",
    "    failure_chunks.extend(text_splitter.split_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637421de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTERING #\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "k = 10  # Number of clusters\n",
    "\n",
    "embedding_parts_vectors = embedding_model.embed_documents(parts_chunks)\n",
    "embedding_parts_vectors = np.array(embedding_parts_vectors).astype('float32')\n",
    "parts_dimension = embedding_parts_vectors.shape[1] \n",
    "\n",
    "embedding_failure_vectors = embedding_model.embed_documents(failure_chunks)\n",
    "embedding_failure_vectors = np.array(embedding_failure_vectors).astype('float32')\n",
    "failure_dimension = embedding_failure_vectors.shape[1] \n",
    "\n",
    "# Create and train the KMeans model\n",
    "kmeans_parts = faiss.Kmeans(d=parts_dimension, k=k, niter=20, verbose=True)\n",
    "kmeans_parts.train(embedding_parts_vectors)\n",
    "\n",
    "kmeans_failure = faiss.Kmeans(d=failure_dimension, k=k, niter=20, verbose=True)\n",
    "kmeans_failure.train(embedding_failure_vectors)\n",
    "\n",
    "# Assign each embedding to a cluster\n",
    "parts_index = kmeans_parts.index\n",
    "failure_index = kmeans_failure.index\n",
    "\n",
    "# Assign each vector to the nearest centroid\n",
    "parts_distances, parts_cluster_ids = parts_index.search(embedding_parts_vectors, 1) # cluster_ids is shape with the cluster label for eahc vector\n",
    "failure_distances, failure_cluster_ids = failure_index.search(embedding_failure_vectors, 1) \n",
    "\n",
    "# Flatten if needed\n",
    "parts_cluster_ids = parts_cluster_ids.flatten()\n",
    "failure_cluster_ids = failure_cluster_ids.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc4e97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EMBEDDINGS ##\n",
    "# Add metadata with cluster IDs!\n",
    "parts_vectorstore = FAISS.from_texts(\n",
    "    texts=parts_chunks,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=[{\"cluster\": int(cid)} for cid in parts_cluster_ids]\n",
    ")\n",
    "\n",
    "failure_vectorstore = FAISS.from_texts(\n",
    "    texts=failure_chunks,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=[{\"cluster\": int(cid)} for cid in failure_cluster_ids]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09ed3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING PDFs #\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"  # Avoid None\n",
    "    return text\n",
    "\n",
    "# Dictionary of PDF names and paths\n",
    "pdf_files = {\n",
    "    \"KXE_brochure\": r\"C:/Users/lgian/OneDrive - James Cook University/Data/998KXE_Brochure.pdf\",\n",
    "    \"XE_brochure\": r\"C:/Users/lgian/OneDrive - James Cook University/Data/998XE_Brochure.pdf\",\n",
    "    \"XE_specs\": r\"C:/Users/lgian/OneDrive - James Cook University/Data/998XE_Specifications.pdf\",\n",
    "}\n",
    "\n",
    "# Extract text for each file\n",
    "pdf_texts = {name: extract_text_from_pdf(path) for name, path in pdf_files.items()}\n",
    "\n",
    "# Split into chunks\n",
    "splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "chunks = []\n",
    "for name, text in pdf_texts.items():\n",
    "    file_chunks = splitter.split_text(text)\n",
    "    # Tag chunks with filename if you like\n",
    "    chunks.extend([f\"{name}: {chunk}\" for chunk in file_chunks])\n",
    "\n",
    "# Create embeddings + cluster using FAISS (kmeans)\n",
    "embedding_pdf_vectors = embedding_model.embed_documents(chunks)\n",
    "embedding_pdf_vectors = np.array(embedding_pdf_vectors).astype('float32')\n",
    "pdf_dimension = embedding_pdf_vectors.shape[1]  \n",
    "kmeans_pdf = faiss.Kmeans(d=pdf_dimension, k=k, niter=20, verbose=True)\n",
    "kmeans_pdf.train(embedding_pdf_vectors)\n",
    "pdf_index = kmeans_pdf.index\n",
    "pdf_distances, pdf_cluster_ids = pdf_index.search(embedding_pdf_vectors, 1)\n",
    "pdf_cluster_ids = pdf_cluster_ids.flatten()\n",
    "\n",
    "\n",
    "pdf_vectorstore = FAISS.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=[{\"cluster\": int(cid)} for cid in pdf_cluster_ids]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206fbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPTING #\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are a reliability engineer. \n",
    "You have the following list of part numbers and descriptions:\n",
    "\n",
    "{parts_context}\n",
    "\n",
    "Task: For the given failure description, return the most relevant PartNumber.\n",
    "If none of the part numbers seem to fit, return \"Unknown\".\n",
    "\n",
    "Use this extra context to make informed decisions: \n",
    "{KXE_brochure_context}, {XE_brochure_context}, {XE_specs_context}\n",
    "\n",
    "Failure: {failure_context}\n",
    "Answer with ONLY the PartNumber or \"Unknown\".\n",
    "\n",
    "Note: Any PM02 work order that is linked to a service (i.e., PM02: 2W Mech Svce CAT 988K XE Loader) \n",
    "will be a scheduled work activity that requires a service kit. \n",
    "If you cannot find a specific spare part for the failure, **return the corresponding service kit**. \n",
    "Available service kits: 2W, 4W, 8W, 16W, and 40W.\n",
    "\n",
    "Please output your thinking process as well as your final answer.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5c24b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIEVER #\n",
    "# Each time the for loop below runs it will get the top 3 similair parts from the parts_vectorstore\n",
    "parts_retriever = parts_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retriever_pdf = pdf_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9be009ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILURE MAPPING #\n",
    "failure_to_part = []\n",
    "spare_parts = []\n",
    "pdf = []\n",
    "\n",
    "for text in failure_list:\n",
    "    # Get top matching parts\n",
    "    top_parts = parts_retriever.get_relevant_documents(text)\n",
    "    parts_context = \"\\n\".join([doc.page_content for doc in top_parts])\n",
    "\n",
    "    top_pdf_parts = retriever_pdf.get_relevant_documents(text)\n",
    "    pdf_context = \"\\n\".join([doc.page_content for doc in top_pdf_parts])\n",
    "    \n",
    "    # Prepare prompt\n",
    "    prompt = prompt_template.format(\n",
    "        parts_context= parts_context, \n",
    "        failure_context=text, \n",
    "        KXE_brochure_context=pdf_context,\n",
    "        XE_brochure_context=pdf_context,\n",
    "        XE_specs_context=pdf_context\n",
    "    )\n",
    "    \n",
    "    # Get LLM response\n",
    "    response = llm.predict(prompt)\n",
    "    \n",
    "    failure_to_part.append({\n",
    "        \"Failure\": text,\n",
    "        \"Mapped Part\": response\n",
    "    })\n",
    "\n",
    "    spare_parts.append({\n",
    "        \"Top Parts\": top_parts\n",
    "    })\n",
    "\n",
    "    pdf.append({\n",
    "        \"Top Parts\": top_pdf_parts\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "    top_parts_df = pd.DataFrame(spare_parts)\n",
    "    top_pdf_parts_df = pd.DataFrame(pdf)\n",
    "    top_parts_df.to_excel(r\"C:/Users/lgian/OneDrive - James Cook University/Data/Top_Parts.xlsx\", index=False)\n",
    "    top_pdf_parts_df.to_excel(r\"C:/Users/lgian/OneDrive - James Cook University/Data/Top_PDF_Parts.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6d4ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_search = pd.DataFrame(failure_to_part)\n",
    "similarity_search.to_excel(r\"C:/Users/lgian/OneDrive - James Cook University/Data/Clustering_Results.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
